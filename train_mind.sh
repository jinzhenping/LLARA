python main.py \
    --dataset mind_data \
    --data_dir data/ref/mind \
    --model_name mlp_projector \
    --loss lm \
    --batch_size 8 \
    --num_workers 8 \
    --lr 1e-3 \
    --accumulate_grad_batches 8 \
    --max_epochs 10 \
    --check_val_every_n_epoch 1 \
    --precision bf16 \
    --accelerator gpu \
    --devices 0 \
    --llm_path ./llama2-7b-hf \
    --rec_model_path ./SASRec.pth \
    --prompt_path ./prompt/news.txt \
    --ckpt_dir ./checkpoints/mind/ \
    --log_dir mind_logs \
    --output_dir ./output/mind/ \
    --rec_embed SASRec \
    --llm_tuning lora \
    --lora_r 8 \
    --lora_alpha 32 \
    --lora_dropout 0.1 \
    --mode train

